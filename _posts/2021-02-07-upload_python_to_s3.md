---
layout: post
title: "Uploading to AWS S3 Bucket from Python file"
subtitle: "First step to saving results from webapp"
date: 2021-02-07 12:09:42PM EST
background: '/img/posts/joel-muniz-8xQJ5LUvBwA-unsplash.jpg'
markdown:           kramdown
category: tech
tags: tech web-app bucket s3 aws 
description: Upload to an AWS S3 bucket with a python function.
photo-cred: Joel Muniz
photo-cred-link: https://unsplash.com/@jmuniz
---

  <a class="top-link hide" href="" id="js-top">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6"><path d="M12 6H0l6-6z"/></svg>
      <span class="screen-reader-text">Back to top</span>
      </a>

<!--
<script src="https://gist.github.com/franktcao/0683211eaf86f419dc8ea2f0eb85960c.js"></script>
-->

# Intro
After realizing that files generated by `heroku` apps are temporary (*duh*), 
there was a need to get those files on something more persistent -- ideally a 
programmatic way with `python`. 

Fortunately, there are many cloud storage options to consider: `AWS`, `GCP`, etc.

For this tutorial, `AWS` `S3` was chosen, using the `boto3` `python` package to set up 
an `S3` client and upload to a bucket.

# Setup
## Create AWS account
To upload to an AWS S3 bucket, you'll need to sign up for an AWS account! Get one here:
[https://aws.amazon.com/](https://aws.amazon.com/)

## Requirements
First, you'll need to add these to your `requirements.txt` (As always, I suggest 
working in a 
[virtual environment](https://franktcao.github.io/tech/2020/07/27/setting_up.html)):
```
awscli==1.19.3
boto3==1.17.3
botocore==1.20.3
```
Note, the pinned versions, likely to be outdated by the time you read this, are just 
to show a current working combination of requirements (of course, you can leave 
these unpinned if you so choose).

## Credentials
In order to know if an upload is coming from a legitimate source, credentials need
to be set up. Let's start by creating, getting, and setting your AWS credentials.


### Create and Get Access Keys
Sign in to your AWS and go to your account's `Security Credentials` (in the top right
taskbar, navigate through `My Acccount` -> `Security Credentials`).

<div style="text-align: center;">
    <img src="/img/posts/upload_python_to_s3/security_credentials.png" width="80%" alt="Security credentials"/>
</div>

Click the `Access keys (access key ID and secret access key)` section to expand it.

Click the blue `Create New Access Key` button. Your key ID and secret have been
generated. Download it and save it to somewhere secure; keep for your records.

<div style="text-align: center;">
    <img src="/img/posts/upload_python_to_s3/create_key.png" width="50%" alt="Create security keys"/>
</div>

<br>

### Set Access Keys
There are several ways `boto3` but the most secure ways to do so are to store your
credentials as environment variables or to add them to your config file (you can
read all of it in the
[docs](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html).
I find the most straightforward way is to store them in a config file.

#### Add credentials to config file
If it doesn't exist already, create a config file named `~/.aws/config` and add your
credentials there:
```bash
[default]
aws_access_key_id=YOUR_ACCESS_KEY_ID
aws_secret_access_key=YOUR_SECRET_ACCESS_KEY
```

where `YOUR_ACCESS_KEY_ID` and `YOUR_SECRETE_ACCESS_KEY` are those values you just 
created. They should respectively look something like:
```bash
AKMAJZBQRYWJX71XMPZA
```
and
```bash
SOiQJImr8c5mxX1VDJssCew0M0kpMsMnS/ypCrl1
```
(Don't worry, these are fake).

These credentials will be used by default for any `boto3` and/or AWS CLI calls. If
you have different credentials for different profiles, you can add them to the same
config file but just a different section names (e.g. `[default]` -> `[profile dev]`.
Note, `profile` is required in there. Consult the docs for more information).

## Create S3 Bucket
Log onto your S3 AWS console: 
[https://s3.console.aws.amazon.com/](https://s3.console.aws.amazon.com/)
and click the `Create Bucket` button. Give the bucket a unique name, select an 
appropriate `Region`, and configure the bucket to your liking. 
<div style="text-align: center;">
    <img src="/img/posts/upload_python_to_s3/create_bucket.png" width="100%" alt="Create 
security keys"/>
</div>

Click the `Create bucket` button at the very bottom of the page to create it. This 
will create the bucket and redirect you to the console where you should now see the 
bucket.

<div style="text-align: center;">
    <img src="/img/posts/upload_python_to_s3/buckets.png" width="80%" alt="Create 
security keys"/>
</div>

Now, you're ready to go!

# Example Usage
## Python Function
The provided function, from the 
[`Boto3` docs](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html),
is well documented but here the function is renamed to distinguish from possibly 
uploading to other services:

```python
import boto3
import logging
from botocore.exceptions import ClientError
from typing import Optional


def upload_to_s3(
    file_name: str,
    bucket: str,
    object_name: Optional[str] = None
) -> bool:
    """Upload a file to an S3 bucket.

    :param file_name: File to upload
    :param bucket: Bucket to upload to
    :param object_name: S3 object name. If not specified then file_name is used
    :return: True if file was uploaded, else False
    """

    # If S3 object_name was not specified, use file_name
    if object_name is None:
        object_name = file_name

    # Upload the file
    s3_client = boto3.client("s3")
    try:
        s3_client.upload_file(file_name, bucket, object_name)
    except ClientError as e:
        logging.error(e)
        return False
    return True
```

## Try it out for yourself!
### Option 1: Jupyter Notebook/Lab
Make sure your workspace/working directory has the file you want to upload somewhere 
in it.

Run `jupyter` `notebook` or `lab` (I personally prefer `lab` since you can have 
`vim` keybindings but that might take some additional steps to set up):
```bash
jupyter notebook
```
Copy the function and imports into a cell and run it. Add a cell with
```python
FULL_PATH_TO_FILE = "some/path/to/file.ext"
YOUR_BUCKET_NAME = "my-superb-bucket"
YOUR_OBJECT_NAME = "my_test_file.ext"
success = upload_to_s3(
    file_name=FULL_PATH_TO_FILE, 
    bucket=YOUR_BUCKET_NAME, 
    object_name=YOUR_OBJECT_NAME
)
if success:
    print(f"Successfully uploaded '{YOUR_OBJECT_NAME}' to '{YOUR_BUCKET_NAME}'!")
else:
    print("Failed to upload.")
```
Run the cell, and you should have your file successfully uploaded! Check your S3 
console to make sure:
[https://s3.console.aws.amazon.com/](https://s3.console.aws.amazon.com/)


### Option 2: Setup for Bigger Project
Let's save function above in a file with utilities: `utils.py`
Then create a file `upload_file.py` and add this to its contents:
```python
from .utils.py import upload_to_s3

if __name__ == "__main__":
    FULL_PATH_TO_FILE = "some/path/to/file.ext"
    YOUR_BUCKET_NAME = "my-superb-bucket"
    YOUR_OBJECT_NAME = "my_test_file.ext"
    success = upload_to_s3(
        file_name=FULL_PATH_TO_FILE,
        bucket=YOUR_BUCKET_NAME,
        object_name=YOUR_OBJECT_NAME
    )
    if success:
        print(f"Successfully uploaded '{YOUR_OBJECT_NAME}' to '{YOUR_BUCKET_NAME}'!")
    else:
        print("Failed to upload.")
```
You can use the `argparse` package to make it more customizable and more of a 
command-line interface (CLI) tool. 

Finally, you can run it in the command line:

```bash
>>> python -m upload_file
Sucessfully uploaded 'my_test_file.ext' to 'my-superb-bucket'!
```

# Conclusion
There you have it! Now you can programmatically upload whatever files you have 
locally *or* whatever files your `heroku` app generates on its servers!
